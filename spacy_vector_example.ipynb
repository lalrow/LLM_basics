{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYWdE2gkBZqs",
        "outputId": "0f37275e-96fc-4989-a8c6-93dcdc2f3279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-30 23:01:45.043613: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-30 23:01:45.043704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-30 23:01:45.045482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-30 23:01:46.873166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Download the medium-sized English model\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "\n",
        "# Download NLTK resources\n",
        "download('stopwords')\n",
        "download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omtj7VlH9ecF",
        "outputId": "b82c2cb3-8130-43c0-8bf8-f0c3b055fe7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Define your sentences\n",
        "original_text = \"The ocean is vast\"\n",
        "synonym_text = \"The ocean is tiny\"\n",
        "original_text_without_stopword = \"The ocean vast\"\n",
        "synonym_without_stopword = \"The ocean tiny\"\n",
        "unrelated_text = \"unrelated\"\n",
        "\n",
        "# Preprocess texts\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Get vectors for the words\n",
        "original = nlp(original_text).vector.reshape(1, -1)\n",
        "synonym = nlp(synonym_text).vector.reshape(1, -1)\n",
        "original_without_stopword_vector = nlp(original_text_without_stopword).vector.reshape(1, -1)\n",
        "synonym_without_stopword_vector = nlp(synonym_without_stopword).vector.reshape(1, -1)\n",
        "unrelated_without_stopword_vector = nlp(unrelated_text).vector.reshape(1, -1)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original, original_without_stopword_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text} and {original_text_without_stopword}: {similarity_score:.4f}\")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original, synonym)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text} and {synonym_text}: {similarity_score:.4f}\")\n",
        "\n",
        "similarity_score = cosine_similarity(original_without_stopword_vector, synonym_without_stopword_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text_without_stopword} and {synonym_without_stopword}: {similarity_score:.4f}\")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original_without_stopword_vector, unrelated_without_stopword_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text_without_stopword} and {unrelated_text}: {similarity_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjra5SHkAu32",
        "outputId": "a1d64709-9be1-475d-be4c-edb37bcf8bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between The ocean is vast and The ocean vast: 0.7983\n",
            "Cosine Similarity between The ocean is vast and The ocean is tiny: 0.9469\n",
            "Cosine Similarity between The ocean vast and The ocean tiny: 0.8872\n",
            "Cosine Similarity between The ocean vast and unrelated: 0.3123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Define your sentences\n",
        "original_text = \"The ocean is vast\"\n",
        "synonym_text = \"The ocean is tiny\"\n",
        "original_text_without_stopword = \"ocean is vast\"\n",
        "synonym_without_stopword = \"The ocean tiny\"\n",
        "unrelated_text = \"unrelated\"\n",
        "\n",
        "# Preprocess texts\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "original_without_stopword = preprocess_text(original_text_without_stopword)\n",
        "synonym_without_stopword = preprocess_text(synonym_without_stopword)\n",
        "unrelated_without_stopword = preprocess_text(unrelated_text)\n",
        "\n",
        "# Get vectors for the words\n",
        "original = nlp(original_text).vector.reshape(1, -1)\n",
        "synonym = nlp(synonym_text).vector.reshape(1, -1)\n",
        "original_without_stopword_vector = nlp(original_without_stopword).vector.reshape(1, -1)\n",
        "synonym_without_stopword_vector = nlp(synonym_without_stopword).vector.reshape(1, -1)\n",
        "unrelated_without_stopword_vector = nlp(unrelated_without_stopword).vector.reshape(1, -1)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original, original_without_stopword_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text} and {original_text_without_stopword}: {similarity_score:.4f}\")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original, synonym)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text} and {synonym_text}: {similarity_score:.4f}\")\n",
        "\n",
        "similarity_score = cosine_similarity(original_without_stopword_vector, synonym_without_stopword_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text_without_stopword} and {synonym_without_stopword}: {similarity_score:.4f}\")\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original_without_stopword_vector, unrelated_without_stopword_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text_without_stopword} and {unrelated_text}: {similarity_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7zVkqBYAvOS",
        "outputId": "af38684d-a743-4a07-f8e6-b70881c09ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between The ocean is vast and ocean is vast: 0.6294\n",
            "Cosine Similarity between The ocean is vast and The ocean is tiny: 0.9469\n",
            "Cosine Similarity between ocean is vast and ocean tiny: 0.7602\n",
            "Cosine Similarity between ocean is vast and unrelated: 0.1861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "\n",
        "# Download NLTK resources\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Define your sentences\n",
        "original_text = \"The ocean is vast\"\n",
        "similar_text = \"ocean is vast\"\n",
        "\n",
        "# Preprocess texts\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply lemmatization\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "    return lemmatized_text\n",
        "\n",
        "original_processed = lemmatize_text(preprocess_text(original_text))\n",
        "similar_processed = lemmatize_text(preprocess_text(similar_text))\n",
        "\n",
        "# Get vectors for the words\n",
        "original_vector = nlp(original_processed).vector.reshape(1, -1)\n",
        "similar_vector = nlp(similar_processed).vector.reshape(1, -1)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_score = cosine_similarity(original_vector, similar_vector)[0, 0]\n",
        "print(f\"Cosine Similarity between {original_text} and {similar_text}: {similarity_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67RzNGVdGhFm",
        "outputId": "793ca2cd-bde2-4a33-8399-436609848a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between The ocean is vast and river is vast: 0.8343\n"
          ]
        }
      ]
    }
  ]
}